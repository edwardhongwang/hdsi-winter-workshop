{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dzeHYa5GCxN7"},"outputs":[],"source":["# MIT License\n","#\n","# @title Copyright (c) 2024 Mauricio Tec { display-mode: \"form\" }\n","\n","# Permission is hereby granted, free of charge, to any person obtaining a\n","# copy of this software and associated documentation files (the \"Software\"),\n","# to deal in the Software without restriction, including without limitation\n","# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","# and/or sell copies of the Software, and to permit persons to whom the\n","# Software is furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","# DEALINGS IN THE SOFTWARE."]},{"cell_type":"markdown","metadata":{"id":"13i7KQ9t-CV8"},"source":["\n","# Welcome to the HDSI Winter Workshop on LLMs as Autonomous Agents\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1q4SGPmn6sWQhskt4D-1D09q_6C9FDz_L\" alt=\"drawing\" width=\"400\"/>\n","\n","\n","# **Part II: Grounding Agents with Fine-tuning and RL**\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/mauriciogtec/hdsi-winter-workshop/blob/main/llm-agents-part1.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","Expected completion time: 1 hour\n","\n","\n","## March 6, 2025  <br> Mauricio Tec\n","\n"]},{"cell_type":"markdown","source":["**TL;DR** Our previous tutorial gave us the tools to understand agentic LLM workflows. In this tutorial we will talk about learning. We will use fine tuning and reinforcement learning to improve the LLM for specific tasks.\n","\n","üî•üî• üìö **Let's learn how to learn** üìöüî•üî•\n","\n","*Familiarity with PyTorch models is assumed.*\n","\n","\n","<br>\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1e3nRrx9IT5BjhWFwk1VKHS-a6m0EoMhc\" alt=\"drawing\" width=\"450\"/>\n","\n","\n","See also:\n","\n","* [Part I: Introduction to Agentic Frameworks](https://colab.research.google.com/github/mauriciogtec/hdsi-winter-workshop/blob/main/llm-agents-part1.ipynb)\n","* [Pre-assignment: Setup LLM Access & API Keys](https://colab.research.google.com/github/mauriciogtec/hdsi-winter-workshop/blob/main/pre-assignment.ipynb)\n"],"metadata":{"id":"rMVhWY7bxfNl"}},{"cell_type":"markdown","source":["## Software Prerequisites & Setup\n"],"metadata":{"id":"BPO_LtqGh2jr"}},{"cell_type":"markdown","source":["### Utility Function: Markdown Printing\n","\n","As in part I, we will define a very simple utility function to print nicely in a colab notebook environment with Markdown. This is not really needed, but it will make some output visualizations easier and nicer.\n"],"metadata":{"id":"ggb8xlQ9uPF9"}},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","def printmd(string):\n","    display(Markdown(string))\n","\n","test = \"`This is code`. *This is italics*. **This is bold**.\"\n","printmd(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":45},"id":"w79rez5guLTw","executionInfo":{"status":"ok","timestamp":1741115537360,"user_tz":300,"elapsed":147,"user":{"displayName":"Mauricio Tec","userId":"14576222252759708863"}},"outputId":"ae8ddb77-8d7c-470d-f72b-5cd04b6e3ac8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"`This is code`. *This is italics*. **This is bold**."},"metadata":{}}]},{"cell_type":"markdown","source":["### Install Requirements\n","\n","* The main tool will be `PyTorch`, which is the most common deep learning research framework.\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" alt=\"drawing\" width=\"300\"/>\n","\n","\n","* Our second main tool, more specific to this tutorial, is the  `transformers` library, which provides access to various open-source `LLMs` as PyTorch. With transformers we have access to their internals, code, and weights.\n","* The `HuggingFace` tool ecosystem includes various other packages that we will need to be able to manipulate such massive models, which go beyond standard neural network training. Examples include `peft`, `bitsandbytes`, `accelerate`. For reinforcement learning and finetuning, we will use the `trl` library, which includes functionality for finetuning.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1RGuWtGHW88vk7T5JyMjnepG9YzwKJEVM\" alt=\"drawing\" width=\"600\"/>\n","\n","\n","* üéÆ Let's play a game! While the techniques we will study apply to many environments. We will base our tutorial one nice text-based game called `TextWorld`.  \n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1fdfrUd4gxsute0b6qRdZ5d6l4y16D6UK\" alt=\"drawing\" width=\"600\"/>\n"],"metadata":{"id":"J4gJrYdPoAXX"}},{"cell_type":"code","source":["%pip install -q \\\n","  transformers[torch,accelerate] \\\n","  trl[peft,quantization] \\\n","  textworld-express"],"metadata":{"id":"-m6L0kyHhzoL","executionInfo":{"status":"ok","timestamp":1741124805532,"user_tz":300,"elapsed":5520,"user":{"displayName":"Mauricio Tec","userId":"14576222252759708863"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"07f8c97e-4b10-411c-d27b-2eb33d344325"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Setup HuggingFace\n","\n","We will be working with the `meta-llama/Llama-3.2-1B-Instruct` model. Let us check access.\n","\n","See also the [pre-assignment notebook](https://colab.research.google.com/github/mauriciogtec/hdsi-winter-workshop/blob/main/pre-assignment.ipynb) for more details on setting up access to Llama 3.2 on HuggingFace.\n",""],"metadata":{"id":"KlIToEbRRP0y"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","# Retrieve open AI key from Colab secrets\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n","\n","# We will be working with"],"metadata":{"id":"T5n4e4h20cpq","executionInfo":{"status":"ok","timestamp":1741144015985,"user_tz":300,"elapsed":248,"user":{"displayName":"Mauricio Tec","userId":"14576222252759708863"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","\n","### ü§ó What did we learn? ü§î\n","\n","* ü™ú In an agentic framework, a problem is solved step by step.\n","* üÜò LLMs are trained for text completion only. Hence, they struggle at simple operations such as counting or arithmetic which are not aligned with the next-token prediction training.\n","* üôá‚Äç‚ôÇÔ∏è They can immediately solve more complex task by *thinking step by step*. We can implement it with the chain-of-thought prompting technique.\n","* üõ†Ô∏è By leveraging their ability to call tools (code or JSON), we can fill the gap in their abilities. We can implement it with a simple react loop, which underlies most agentic frameworks.\n","\n","\n","### ‚û°Ô∏è Next steps ‚û°Ô∏è\n","\n","* üíæ More sophisticated memory: in the examples, we simply use the thought, observation, action history as the agent's memory. But for long sequences, we can use a RAG agent. Remember, each token costs money\n","\n","* üëØ So far, we have approached the problem in a single-agent way. But many agentic frameworks allow to have multiple agents. A simple design is having an orchestrator agent which uses other agents as tools, but there are many use cases and designs.\n","\n","* ü§ñ In the next part of the tutorial, we will cover how to improve an agent performance with fine tuning and reinforcement learning.\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1gA9lNXqJunfai38RS6DSRenuXKFysHW6\" alt=\"drawing\" width=\"400\"/>\n"],"metadata":{"id":"hPFpzaD_gyOM"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1VpgUrL9b971Zx3TIGnlDuDuD8_8aSZo2","timestamp":1740795910938},{"file_id":"17oQqcbIJeM3EIruP4T2ju_-LNQmuqqYg","timestamp":1740539043540},{"file_id":"1rKHeTu4U_CU87_Kwt4MI7yrL7VGHW9hS","timestamp":1717121824388},{"file_id":"https://github.com/mauriciogtec/vlmrl/blob/main/vlmrl.ipynb","timestamp":1713063913073}],"machine_shape":"hm","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}